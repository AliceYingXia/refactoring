{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Python Enhancement Proposal: Stuyle Guide for Python Code",
   "id": "a2e2d1672755d63e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 1 – Python Syntax & Naming Conventions**",
   "id": "4129408540c4c73c"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "import os,sys\n",
    "\n",
    "def Load_Data(FilePath:str)->list:\n",
    "    \"load jsonl file and return list of lines\"\n",
    "    if FilePath == None:\n",
    "        raise ValueError(\"filepath can not be None\")\n",
    "\n",
    "    data=[]\n",
    "    with open(FilePath, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if len(line.strip()) == 0: continue\n",
    "            data.append( line )\n",
    "\n",
    "    print(\"loaded %s lines from file\" % len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "def filterlines(lines, Min_Length =10):\n",
    "    \"filter by min length\"\n",
    "    result = []\n",
    "    for l in lines:\n",
    "        if len(l) > Min_Length:\n",
    "            result.append(l)\n",
    "    return result\n"
   ],
   "id": "db6eb1e96b34f7a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def load_data(file_path:str)->list[str]:\n",
    "    \"load jsonl file and return list of lines\"\n",
    "    if file_path is None:\n",
    "        raise ValueError(f\"{file_path} can not be None\")\n",
    "\n",
    "    data=[]\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if len(line.strip()) == 0: continue\n",
    "            data.append(line)\n",
    "\n",
    "    print(f\"loaded {len(data)} lines from file\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def filter_lines(lines: list[str], min_Length =10)->list[str]:\n",
    "    \"filter by min length\"\n",
    "    return [ l for l in lines if len(l) > min_Length ]\n"
   ],
   "id": "d40398c2c2af1836"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 2 – Typos & Buggy Logic**",
   "id": "ca72821c88e56f59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "def normalize_scores(scors: list[float]) -> list[float]:\n",
    "    \"Normalize a list of scores to 0-1 range\"\n",
    "    min_score = min(scors)\n",
    "    max_score = max(score for score in scors)\n",
    "    span = max_score - min_score\n",
    "\n",
    "    if span == 0:\n",
    "        return [0.0 for s in scores]\n",
    "\n",
    "    normd = []\n",
    "    for score in scores:\n",
    "        norm = (scor - min_score) / span\n",
    "        normd.append(norm)\n",
    "\n",
    "    retrun normd\n"
   ],
   "id": "e04e823d272f9320"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "from typing import List\n",
    "\n",
    "def normalize_scores(scores: List[float]) -> List[float]:\n",
    "    \"Normalize a list of scores to 0-1 range\"\n",
    "    if not scores:\n",
    "        raise ValueError(\"scores can not be None\")\n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    span = max_score - min_score\n",
    "\n",
    "    if span == 0:\n",
    "        return [0.0 for _ in scores]\n",
    "\n",
    "    return [(s - min_score)/span for s in scores]\n"
   ],
   "id": "6c745234896533ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 3 – Async Syntax & Blocking Code**",
   "id": "a60c08993be38518"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "async def fetch_user_profile(user_id: str) -> dict:\n",
    "    \"\"\"Fake network call\"\"\"\n",
    "    time.sleep(0.5)  # simulate IO\n",
    "    return {\"userId\": user_id, \"status\": \"ok\"}\n",
    "\n",
    "async def fetch_all_users(ids: List[str]) -> list[dict]:\n",
    "    profiles = []\n",
    "    for id in ids:\n",
    "        profile = fetch_user_profile(id)\n",
    "        profiles.append(profile)\n",
    "    return await asyncio.gather(*profiles)\n",
    "\n",
    "async def main():\n",
    "    users = [\"u1\", \"u2\", \"u3\"]\n",
    "    results = await fetch_all_users(users)\n",
    "    print(\"done:\", results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(main())\n",
    "    loop.close()\n"
   ],
   "id": "9111686c94ed67d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "import asyncio\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "async def fetch_user_profile(user_id: str) -> Dict[str, str]:\n",
    "    \"\"\"Fake network call\"\"\"\n",
    "    # time.sleep(0.5)  # simulate IO\n",
    "    await asyncio.sleep(0.5)\n",
    "    return {\"userId\": user_id, \"status\": \"ok\"}\n",
    "\n",
    "async def fetch_all_users(ids: List[str]) -> List[Dict[str, str]]:\n",
    "    profiles = [fetch_user_profile(id) for id in ids]\n",
    "    return await asyncio.gather(*profiles)\n",
    "\n",
    "async def main():\n",
    "    users = [\"u1\", \"u2\", \"u3\"]\n",
    "    results = await fetch_all_users(users)\n",
    "    print(\"done:\", results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "id": "96d478fbf7fe82b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 4 – Mistral API (Sync) Review**",
   "id": "59a55105906665f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "def call_mistral(prompt: str) -> str:\n",
    "    client = Mistral(api_key=os.environ[\"MISTRAL_KEY\"])\n",
    "    model_name = \"mistral-medium-latest\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message[\"content\"]\n"
   ],
   "id": "937e5818a4950226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "import os\n",
    "from typing import List\n",
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(api_key=os.environ[\"MISTRAL_KEY\"])\n",
    "\n",
    "DEFAULT_MODEL = \"mistral-medium-latest\"\n",
    "\n",
    "def call_mistral(prompt: str, model_name: str = DEFAULT_MODEL) -> str:\n",
    "\n",
    "    response = client.chat.complete(\n",
    "        model=model_name,\n",
    "        message=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ],
   "id": "a30ad0999a117330"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 5 – Mistral API (Async) + Async Pitfalls**",
   "id": "e25ccace89351199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "import os\n",
    "import asyncio\n",
    "from mistralai import Mistral\n",
    "\n",
    "async def chat_once(user_msg: str) -> str:\n",
    "    client = Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\", \"\"))\n",
    "\n",
    "    async with client:\n",
    "        res = client.chat.complete_async(\n",
    "            model=\"mistral-small-latest\",\n",
    "            messages={\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            },\n",
    "            stream=False\n",
    "        )\n",
    "        return res.choices[0].message.content\n",
    "\n",
    "def main():\n",
    "    answer = asyncio.run(chat_once(\"hello!\"))\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.get_event_loop().run_until_complete(main())\n"
   ],
   "id": "fb427ba15f60a48b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "import os\n",
    "import asyncio\n",
    "from mistralai import Mistral\n",
    "\n",
    "DEFAULT_MODEL = \"mistral-small-latest\"\n",
    "\n",
    "async def chat_once(user_msg: str, model: str = DEFAULT_MODEL) -> str:\n",
    "\n",
    "    async with Mistral(api_key=os.getenv(\"MISTRAL_API_KEY\", \"\")) as client:\n",
    "        res =  await client.chat.complete_async(\n",
    "            model=DEFAULT_MODEL,\n",
    "            messages=[\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "                }\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        return res.choices[0].message.content\n",
    "\n",
    "async def main():\n",
    "    answer = await chat_once(\"hello!\")\n",
    "    print(answer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ],
   "id": "ba38f5f947a562dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 1 – Messy Data Pipeline + Confusing Types**",
   "id": "83db2d6e5316a9fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "# data_pipeline.py\n",
    "\n",
    "import json,os,asyncio,time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "async def load_json_file(path:str)->list:\n",
    "    f = open(path)\n",
    "    txt = f.read()\n",
    "    lines = txt.split(\"\\n\")\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        if l.strip()==\"\":\n",
    "            continue\n",
    "        try:\n",
    "            data.append(json.loads(l))\n",
    "        except:\n",
    "            print(\"bad json:\", l)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "def filter_users(objs, minage=18, maxage=99, country=None):\n",
    "    res=[]\n",
    "    for x in objs:\n",
    "        # age can be str\n",
    "        age = int(x.get(\"age\", 0))\n",
    "        if age<minage or age>maxage:\n",
    "            continue\n",
    "        if country and x[\"country\"]!=country:\n",
    "            continue\n",
    "        res.append(x)\n",
    "    return res\n",
    "\n",
    "async def enrich_with_delay(users:List[Dict[str,Any]]):\n",
    "    enriched=[]\n",
    "    for u in users:\n",
    "        time.sleep(0.05)  # simulate external service\n",
    "        u[\"score\"]=u.get(\"score\",0)+1\n",
    "        enriched.append(u)\n",
    "    return enriched\n",
    "\n",
    "async def run(path, country):\n",
    "    # can be called inside FastAPI endpoint\n",
    "    data = await load_json_file(path)\n",
    "    filtered = filter_users(data, country=country)\n",
    "    result = await enrich_with_delay(filtered)\n",
    "    return result\n"
   ],
   "id": "3a54c2ecce7fcb4a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "# data_pipeline.py\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from typing import Dict, List, Any, Optional, Iterable\n",
    "\n",
    "def load_json_file(path:Optional[str])->List[Dict[str,Any]]:\n",
    "    if not path:\n",
    "        raise ValueError('Please provide a file path!')\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        records = []\n",
    "        for line_no, line in enumerate(f, start=1):\n",
    "            line = line.strip()\n",
    "            if line==\"\":\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "            except json.decoder.JSONDecodeError:\n",
    "                print(f'bad json in line {line_no}: {line}')\n",
    "                continue\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def filter_users(records: Iterable[Dict[str, Any]], minage:int=18, maxage:int =99, country:Optional[str] =None)->List[Dict[str,Any]]:\n",
    "    res=[]\n",
    "    for record in records:\n",
    "        # age can be str\n",
    "        age = record.get(\"age\", 0)\n",
    "        try:\n",
    "            age = int(age)\n",
    "        except:\n",
    "            age = 0\n",
    "        if age<minage or age>maxage:\n",
    "            continue\n",
    "        if country and record.get(\"country\") !=country:\n",
    "            continue\n",
    "        res.append(record)\n",
    "    return res\n",
    "\n",
    "async def enrich_with_delay(users:List[Dict[str,Any]])->List[Dict[str,Any]]:\n",
    "    enriched=[]\n",
    "    for u in users:\n",
    "        await asyncio.sleep(0.05)  # simulate external service\n",
    "        score = u.get(\"score\",0)\n",
    "        try:\n",
    "            u[\"score\"] = int(score) + 1\n",
    "        except:\n",
    "            u[\"score\"] = 1\n",
    "        enriched.append(u)\n",
    "    return enriched\n",
    "\n",
    "async def run(path:str, country:str=None)->List[Dict[str,Any]]:\n",
    "    # can be called inside FastAPI endpoint\n",
    "    data = load_json_file(path)\n",
    "    filtered = filter_users(data, country=country)\n",
    "    result = await enrich_with_delay(filtered)\n",
    "    return result\n"
   ],
   "id": "d4589ae22f697300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# async file reading\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "import aiofiles\n",
    "\n",
    "\n",
    "async def load_json_file(path: str) -> List[Dict[str, Any]]:\n",
    "    if not path:\n",
    "        raise ValueError(\"Please provide a non-empty file path!\")\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "\n",
    "    async with aiofiles.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        line_no = 0\n",
    "        async for line in f:\n",
    "            line_no += 1\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                records.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Bad JSON on line {line_no}: {line!r}\")\n",
    "\n",
    "    return records\n"
   ],
   "id": "e12143271141a752"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 2 – “Helper” Mistral Wrapper That’s Dangerous in Production**",
   "id": "599b9b664bb48694"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "# mistral_helper_before.py\n",
    "import os, traceback\n",
    "from mistralai import Mistral\n",
    "\n",
    "def ask_mistral(prompt: str, system_prompt: str = None):\n",
    "    client = Mistral(api_key=os.getenv(\"MISTRAL_KEY\"))\n",
    "    try:\n",
    "        if system_prompt:\n",
    "            msgs = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        else:\n",
    "            msgs = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        print(\"sending to model:\", msgs)\n",
    "\n",
    "        resp = client.chat.complete(\n",
    "            model=\"mistral-tiny-latest\",\n",
    "            messages=msgs,\n",
    "            temperature=0.9,\n",
    "            max_tokens=4096,\n",
    "            safe_mode=False\n",
    "        )\n",
    "\n",
    "        return resp.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"ERROR:\", e)\n",
    "        traceback.print_exc()\n",
    "        return str(e)\n"
   ],
   "id": "ad989c13a51ca281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "\"\"\"\n",
    "mistral_helper_before.py\n",
    "\n",
    "Small educational wrapper around the Mistral Python client.\n",
    "\n",
    "- Shows how to:\n",
    "    * structure a client class\n",
    "    * wrap low-level exceptions in your own error type\n",
    "    * build messages\n",
    "    * add basic logging and configuration\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from mistralai import Mistral\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration constants\n",
    "# ----------------------------\n",
    "\n",
    "# Default model you want to call. You can change this in one place.\n",
    "DEFAULT_MODEL = \"mistral-tiny-latest\"\n",
    "\n",
    "# Environment variable name for the API key.\n",
    "# You can configure this in your shell / .env file.\n",
    "API_KEY_ENV = \"MISTRAL_API_KEY\"\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Custom error type\n",
    "# ----------------------------\n",
    "\n",
    "class MistralClientError(Exception):\n",
    "    \"\"\"\n",
    "    High-level wrapper for errors when calling the Mistral API.\n",
    "\n",
    "    Why define this?\n",
    "    - You usually don't want to leak raw SDK or HTTP exceptions all over your code.\n",
    "    - A custom error type lets you:\n",
    "        * add context (which model? what kind of request?)\n",
    "        * catch it specifically in higher-level code: `except MistralClientError: ...`\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    message : str\n",
    "        Human-readable description of the error.\n",
    "    original_exc : BaseException | None\n",
    "        The underlying exception raised by the SDK / HTTP client, if any.\n",
    "    model : str | None\n",
    "        Model name used for the request (useful to debug config issues).\n",
    "    request_payload : dict | None\n",
    "        Optional metadata snapshot (e.g. model + number of messages).\n",
    "        NOTE: In real production code, be careful not to include PII or secrets here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        message: str,\n",
    "        *,\n",
    "        original_exc: Optional[BaseException] = None,\n",
    "        model: Optional[str] = None,\n",
    "        request_payload: Optional[dict] = None,\n",
    "    ) -> None:\n",
    "        # Call base Exception __init__ so str(e) works.\n",
    "        super().__init__(message)\n",
    "\n",
    "        self.message = message\n",
    "        self.original_exc = original_exc\n",
    "        self.model = model\n",
    "        self.request_payload = request_payload\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        String representation used when you print the exception.\n",
    "        We add some extra context if available.\n",
    "        \"\"\"\n",
    "        base = self.message\n",
    "\n",
    "        if self.model:\n",
    "            base += f\" [model={self.model}]\"\n",
    "\n",
    "        if self.original_exc is not None:\n",
    "            base += f\" (caused by {type(self.original_exc).__name__}: {self.original_exc})\"\n",
    "\n",
    "        return base\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Mistral client wrapper\n",
    "# ----------------------------\n",
    "\n",
    "class MistralClient:\n",
    "    \"\"\"\n",
    "    Simple wrapper around `mistralai.Mistral` for chat completion.\n",
    "\n",
    "    Responsibilities:\n",
    "    - Own the underlying `Mistral` SDK client.\n",
    "    - Build messages from prompt + optional system prompt.\n",
    "    - Handle common options (temperature, max_tokens, safe_mode).\n",
    "    - Wrap errors into `MistralClientError`.\n",
    "\n",
    "    This is a *sync* wrapper (using `client.chat.complete`).\n",
    "    You could add an async version later if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key: Optional[str] = None,\n",
    "        model: str = DEFAULT_MODEL,\n",
    "        log_prompts: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        api_key : str | None\n",
    "            The Mistral API key. If None, we'll read it from `API_KEY_ENV`.\n",
    "        model : str\n",
    "            Default model name to use for requests.\n",
    "        log_prompts : bool\n",
    "            Whether to log prompt metadata (NOT the full prompt) to stdout.\n",
    "        \"\"\"\n",
    "        # Prefer explicit `api_key`, fall back to environment variable.\n",
    "        self.api_key = api_key or os.getenv(API_KEY_ENV)\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                f\"Mistral API key not found. Please pass `api_key` or set env {API_KEY_ENV}\"\n",
    "            )\n",
    "\n",
    "        self.model = model\n",
    "        self.log_prompts = log_prompts\n",
    "\n",
    "        # Create underlying SDK client once and reuse it.\n",
    "        self._client = Mistral(api_key=self.api_key)\n",
    "\n",
    "    # ------------- internal helper -------------\n",
    "\n",
    "    def _build_messages(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: Optional[str],\n",
    "    ) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Convert a user prompt + optional system prompt into\n",
    "        the `messages=[...]` format expected by the Mistral chat API.\n",
    "\n",
    "        Example output:\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "        ]\n",
    "        \"\"\"\n",
    "        messages: List[Dict[str, str]] = []\n",
    "\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    # ------------- public method -------------\n",
    "\n",
    "    def ask(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_prompt: Optional[str] = None,\n",
    "        *,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: int = 1024,\n",
    "        safe_mode: bool = True,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Send a prompt (and optional system prompt) to Mistral and return the text response.\n",
    "\n",
    "        This is a convenience method for simple single-turn calls.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt : str\n",
    "            The user prompt.\n",
    "        system_prompt : str | None\n",
    "            Optional system message that sets behavior or instructions.\n",
    "        temperature : float\n",
    "            Sampling temperature; higher is more random.\n",
    "        max_tokens : int\n",
    "            Maximum number of tokens to generate.\n",
    "        safe_mode : bool\n",
    "            Whether to enable safety filters (depends on Mistral's API behavior).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The model's response text.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        MistralClientError\n",
    "            If there is any problem calling the Mistral API.\n",
    "        \"\"\"\n",
    "        messages = self._build_messages(prompt, system_prompt)\n",
    "\n",
    "        if self.log_prompts:\n",
    "            # In real code, use logging instead of print, and avoid logging full text for PII.\n",
    "            print(\n",
    "                \"[MistralClient] Sending chat completion request:\",\n",
    "                f\"model={self.model}, num_messages={len(messages)}\",\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # Call the underlying SDK.\n",
    "            # NOTE: This is sync; for async you would use `await client.chat.complete_async(...)`.\n",
    "            resp = self._client.chat.complete(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                safe_mode=safe_mode,\n",
    "            )\n",
    "        except Exception as exc:\n",
    "            # Build a small payload with non-sensitive context for debugging.\n",
    "            payload_meta = {\n",
    "                \"model\": self.model,\n",
    "                \"temperature\": temperature,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"safe_mode\": safe_mode,\n",
    "                \"num_messages\": len(messages),\n",
    "            }\n",
    "\n",
    "            # Wrap low-level exception in our custom error type.\n",
    "            # The `from exc` keeps traceback chaining (`__cause__`).\n",
    "            raise MistralClientError(\n",
    "                \"Error calling Mistral chat completion\",\n",
    "                original_exc=exc,\n",
    "                model=self.model,\n",
    "                request_payload=payload_meta,\n",
    "            ) from exc\n",
    "\n",
    "        # Extract the text content from the first choice.\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (manual test)\n",
    "# ----------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    If you run this file directly:\n",
    "    - make sure you have `MISTRAL_API_KEY` set in your environment\n",
    "    - it will send a small prompt and print the reply\n",
    "    \"\"\"\n",
    "    client = MistralClient(log_prompts=True)\n",
    "\n",
    "    try:\n",
    "        answer = client.ask(\"Say hello in one short sentence.\")\n",
    "        print(\"Model answer:\", answer)\n",
    "    except MistralClientError as e:\n",
    "        # Here you see how your custom error behaves.\n",
    "        print(\"MistralClientError occurred:\", e)\n",
    "        if e.request_payload:\n",
    "            print(\"Debug payload:\", e.request_payload)\n"
   ],
   "id": "452ce02841e44d3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 3 – Async + CPU Work + Race Conditions**",
   "id": "649b5a23ac0074fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "# scoring_service.py\n",
    "\n",
    "import asyncio, math\n",
    "from typing import List, Dict\n",
    "\n",
    "async def slow_score(user: Dict) -> float:\n",
    "    \"\"\"Fake scoring logic mixing IO and CPU.\"\"\"\n",
    "    # simulate external feature fetch\n",
    "    await asyncio.sleep(0.1)\n",
    "    base = user.get(\"base\", 0)\n",
    "    try:\n",
    "        base = int(base)\n",
    "    except:\n",
    "        raise ValueError(f'{base} cannot be converted to int')\n",
    "    # heavy-ish CPU work\n",
    "    s = 0\n",
    "    for i in range(1, 100000):\n",
    "        s += math.log(i + base + 1)\n",
    "    return s\n",
    "\n",
    "async def compute_scores(users: List[Dict]) -> None:\n",
    "    tasks = {}\n",
    "    for u in users:\n",
    "        uid = u.get(\"id\")\n",
    "        if not uid or uid in GLOBAL_CACHE:\n",
    "            continue\n",
    "        # BUG: no isolation\n",
    "        GLOBAL_CACHE[uid] = 0.0\n",
    "        t = await slow_score(u)\n",
    "        tasks[uid] = t\n",
    "    return tasks\n",
    "\n",
    "    # gather but ignore order\n",
    "    for uid, t in tasks:\n",
    "        GLOBAL_CACHE[uid] = await t\n",
    "\n",
    "async def main():\n",
    "    users = [{\"id\":\"u1\",\"base\":3},{\"id\":\"u2\",\"base\":5},{\"id\":\"u1\",\"base\":7}]\n",
    "    res = await compute_scores(users)\n",
    "    print(res)\n"
   ],
   "id": "47bfbb5783c9c59d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "# scoring_service.py\n",
    "\n",
    "import asyncio, math\n",
    "from typing import List, Dict\n",
    "\n",
    "def base_score_process(base: int)->float:\n",
    "    s = 0\n",
    "    for i in range(1, 100000):\n",
    "        s += math.log(i + base + 1)\n",
    "    return s\n",
    "\n",
    "async def slow_score(user: Dict[str, Any]) -> float:\n",
    "    \"\"\"Fake scoring logic mixing IO and CPU.\"\"\"\n",
    "    # simulate external feature fetch\n",
    "    await asyncio.sleep(0.1)\n",
    "    base = user.get(\"base\", 0)\n",
    "    # heavy-ish CPU work\n",
    "    res = await asyncio.to_thread(base_score_process, base)\n",
    "    return res\n",
    "\n",
    "async def compute_scores(users: List[Dict]) -> Dict[str, float]:\n",
    "    res = {}\n",
    "    uid_list = []\n",
    "    score_list = []\n",
    "    for u in users:\n",
    "        uid = u[\"id\"]\n",
    "        uid_list.append(uid)\n",
    "        score_list.append(asyncio.create_task(slow_score(u)))\n",
    "    score_res = await asyncio.gather(*score_list, return_exceptions=True)\n",
    "    for uid, score in zip(uid_list, score_res):\n",
    "        if isinstance(score, Exception):\n",
    "            print(f'cannot compute score for {uid}: {score}')\n",
    "            res[uid] = 0.0\n",
    "        else:\n",
    "            res[uid] = score\n",
    "    return res\n",
    "\n",
    "async def main():\n",
    "    users = [{\"id\":\"u1\",\"base\":3},{\"id\":\"u2\",\"base\":5},{\"id\":\"u3\",\"base\":7}]\n",
    "    res = await compute_scores(users)\n",
    "    print(res)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n"
   ],
   "id": "79d9af2043a63558"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 4 – Dataclass Misuse, Defaults, and Type Weirdness**",
   "id": "8143ed83b62ebc83"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# before\n",
    "# models.py\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class ChatSession:\n",
    "    userId: str\n",
    "    history: List[Dict[str, str]] = []\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "    system_prompt: str = None\n",
    "    temperature: float = 0.8\n",
    "\n",
    "    def add_utt(self, role, content):\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def trim(self, n):\n",
    "        \"\"\"Keep last n messages.\"\"\"\n",
    "        if n<0:\n",
    "            return\n",
    "        self.history = self.history[-n:]\n",
    "\n",
    "    def to_mistral_messages(self):\n",
    "        msgs = []\n",
    "        if self.system_prompt:\n",
    "            msgs.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
    "        msgs += self.history\n",
    "        return msgs\n"
   ],
   "id": "7ce52fe42d7b3768"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# after\n",
    "# models.py\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Literal\n",
    "\n",
    "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
    "\n",
    "@dataclass\n",
    "class ChatSession:\n",
    "    user_id: str\n",
    "    history: List[Dict[str, str]] = field(default_factory=list)\n",
    "    meta: Dict[str, Any] = field(default_factory=dict)\n",
    "    system_prompt: Optional[str] = None\n",
    "    temperature: float = 0.8\n",
    "\n",
    "    def add_utterance(self, role:Role, content:str)->None:\n",
    "        if role not in ('user', 'assistant', 'user'):\n",
    "            raise ValueError('role can only be \\\"system\\\", \\\"assistant\\\" or \\\"user\\\"!')\n",
    "        self.history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def trim_history(self, n:int)->None:\n",
    "        \"\"\"Keep last n messages.\"\"\"\n",
    "        if n<0:\n",
    "            return\n",
    "        self.history = self.history[-n:]\n",
    "\n",
    "    def to_mistral_messages(self)->List[Dict[str, Any]]:\n",
    "        msgs: List[Dict[str, Any]] = []\n",
    "        if self.system_prompt:\n",
    "            msgs.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
    "        msgs += self.history\n",
    "        return msgs\n"
   ],
   "id": "617f78a5c03b8c11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 1 – Async LLM Client Class (Messy)**",
   "id": "7c7f231e98e1daa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# llm_client.py (junior version)\n",
    "\n",
    "import os, asyncio, time\n",
    "from typing import Any\n",
    "from mistralai import Mistral\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, api_key: str | None = None, model=\"mistral-tiny-latest\"):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        # create client immediately\n",
    "        self.client = Mistral(api_key=self.api_key)\n",
    "        self.total_calls = 0\n",
    "        self.last_response: Any = None\n",
    "\n",
    "    async def _sleep(self, seconds: float):\n",
    "        # simulate network jitter (BAD: blocking)\n",
    "        time.sleep(seconds)\n",
    "\n",
    "    async def make_messages(self, prompt: str, system: str | None = None):\n",
    "        msgs = []\n",
    "        if system is not None:\n",
    "            msgs.append({\"role\": \"system\", \"content\": system})\n",
    "        msgs.append({\"role\": \"user\", \"content\": prompt})\n",
    "        return msgs\n",
    "\n",
    "    async def chat(self, prompt: str, system: str | None = None) -> str:\n",
    "        # randomly sleep a bit\n",
    "        await self._sleep(0.1)\n",
    "\n",
    "        msgs = await self.make_messages(prompt, system)\n",
    "\n",
    "        try:\n",
    "            # forgot to await\n",
    "            resp = self.client.chat.complete_async(\n",
    "                model=self.model,\n",
    "                messages=msgs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"LLM error:\", e)\n",
    "            return \"ERROR\"\n",
    "\n",
    "        self.total_calls += 1\n",
    "        self.last_response = resp\n",
    "        # assume dict-style\n",
    "        return resp.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "async def main():\n",
    "    c = LLMClient()\n",
    "    ans = await c.chat(\"hello\", \"You are helpful.\")\n",
    "    print(ans)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "id": "13b91570524254f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# llm_client.py (junior version)\n",
    "\n",
    "import os, asyncio, time\n",
    "from typing import Any, Optional, Dict, Optional\n",
    "from mistralai import Mistral\n",
    "\n",
    "DEFAULT_MODEL = \"mistral-tiny-latest\"\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, api_key: Optional[str] = None, model:str=DEFAULT_MODEL)->None:\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "        self.api_key = api_key\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY must be set!\")\n",
    "        self.model = model\n",
    "        # create client immediately\n",
    "        self.client = Mistral(api_key=self.api_key)\n",
    "        self.total_calls:int = 0\n",
    "\n",
    "    async def sleep(self, seconds: float)->None:\n",
    "        # simulate network jitter (BAD: blocking)\n",
    "        await asyncio.sleep(seconds)\n",
    "\n",
    "    def make_messages(self, prompt: str, system: Optional[str] = None)->List[Dict[str, Any]]:\n",
    "        msgs: List[Dict[str, Any]] = []\n",
    "        if system is not None:\n",
    "            msgs.append({\"role\": \"system\", \"content\": system})\n",
    "        msgs.append({\"role\": \"user\", \"content\": prompt})\n",
    "        return msgs\n",
    "\n",
    "    async def chat(self, prompt: str, system: Optional[str] = None) -> str:\n",
    "        # randomly sleep a bit\n",
    "        await self.sleep(0.1)\n",
    "\n",
    "        msgs = self.make_messages(prompt, system)\n",
    "\n",
    "        try:\n",
    "            # forgot to await\n",
    "            resp = await self.client.chat.complete_async(\n",
    "                model=self.model,\n",
    "                messages=msgs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"LLM error:\", e)\n",
    "            return \"ERROR\"\n",
    "\n",
    "        self.total_calls += 1\n",
    "        # assume dict-style\n",
    "        return resp.choices[0].message.content\n",
    "\n",
    "\n",
    "async def main():\n",
    "    c = LLMClient()\n",
    "    ans = await c.chat(\"hello\", \"You are helpful.\")\n",
    "    print(ans)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ],
   "id": "7f3a4122d5aa1a1a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Exercise 2 – FastAPI + Async Client + Session Class**",
   "id": "60884096a1bf4bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# session_service.py (junior version)\n",
    "\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from mistralai import Mistral\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# global stuff\n",
    "SESSIONS: Dict[str, \"Session\"] = {}\n",
    "client = Mistral(api_key=\"\")  # FIXME: empty key\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_message: str\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    session_id: str\n",
    "    reply: str\n",
    "    history: list[dict]\n",
    "\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, session_id: str):\n",
    "        self.id = session_id\n",
    "        self.history: list[dict] = []\n",
    "        self.model = \"mistral-tiny-latest\"\n",
    "\n",
    "    async def add_user_message(self, content: str):\n",
    "        self.history.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    async def add_assistant_message(self, content: str):\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    async def to_messages(self):\n",
    "        # no system prompt support, but could add later\n",
    "        return self.history\n",
    "\n",
    "    async def call_model(self) -> str:\n",
    "        msgs = await self.to_messages()\n",
    "        # missing await\n",
    "        resp = client.chat.complete_async(\n",
    "            model=self.model,\n",
    "            messages=msgs,\n",
    "        )\n",
    "        # assume dict style\n",
    "        return resp.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "async def get_or_create_session(session_id: str) -> Session:\n",
    "    if session_id in SESSIONS:\n",
    "        return SESSIONS[session_id]\n",
    "    s = Session(session_id)\n",
    "    SESSIONS[session_id] = s\n",
    "    return s\n",
    "\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(req: ChatRequest):\n",
    "    # BAD: manually manage event loop inside FastAPI\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    session = await get_or_create_session(req.session_id)\n",
    "    await session.add_user_message(req.user_message)\n",
    "\n",
    "    # weird: use run_until_complete in async endpoint\n",
    "    reply = loop.run_until_complete(session.call_model())\n",
    "    await session.add_assistant_message(reply)\n",
    "\n",
    "    return ChatResponse(\n",
    "        session_id=session.id,\n",
    "        reply=reply,\n",
    "        history=session.history,\n",
    "    )\n"
   ],
   "id": "1ff2dc971f3b2419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# session_service.py (junior version)\n",
    "\n",
    "import asyncio\n",
    "from typing import Dict, Any\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from mistralai import Mistral\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# global stuff\n",
    "SESSIONS: Dict[str, \"Session\"] = {}\n",
    "client = Mistral(api_key=\"\")  # FIXME: empty key\n",
    "\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str\n",
    "    user_message: str\n",
    "\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    session_id: str\n",
    "    reply: str\n",
    "    history: list[dict]\n",
    "\n",
    "DEFAULT_MODEL = \"mistral-tiny-latest\"\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, session_id: str, model:str=DEFAULT_MODEL)->None:\n",
    "        self.id = session_id\n",
    "        self.history: List[Dict[str, str]] = []\n",
    "        self.model = model\n",
    "\n",
    "    def add_user_message(self, content: str)->None:\n",
    "        self.history.append({\"role\": \"user\", \"content\": content})\n",
    "\n",
    "    def add_assistant_message(self, content: str)->None:\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": content})\n",
    "\n",
    "    def to_messages(self)->List[Dict[str, str]]:\n",
    "        # no system prompt support, but could add later\n",
    "        return self.history\n",
    "\n",
    "    async def call_model(self) -> str:\n",
    "        msgs: List[Dict[str, str]] = self.to_messages()\n",
    "        # missing await\n",
    "        resp = client.chat.complete_async(\n",
    "            model=self.model,\n",
    "            messages=msgs,\n",
    "        )\n",
    "        # assume dict style\n",
    "        return resp.choices[0].message[\"content\"]\n",
    "\n",
    "\n",
    "async def get_or_create_session(session_id: str) -> Session:\n",
    "    if session_id in SESSIONS:\n",
    "        return SESSIONS[session_id]\n",
    "    s = Session(session_id)\n",
    "    SESSIONS[session_id] = s\n",
    "    return s\n",
    "\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(req: ChatRequest):\n",
    "    # BAD: manually manage event loop inside FastAPI\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    session = await get_or_create_session(req.session_id)\n",
    "    await session.add_user_message(req.user_message)\n",
    "\n",
    "    # weird: use run_until_complete in async endpoint\n",
    "    reply = loop.run_until_complete(session.call_model())\n",
    "    await session.add_assistant_message(reply)\n",
    "\n",
    "    return ChatResponse(\n",
    "        session_id=session.id,\n",
    "        reply=reply,\n",
    "        history=session.history,\n",
    "    )\n"
   ],
   "id": "d8910b315ab833f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
